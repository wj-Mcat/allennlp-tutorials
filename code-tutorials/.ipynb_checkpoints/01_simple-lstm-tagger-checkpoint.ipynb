{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# allennlp-simple-lstm-tagger-tutorials\n",
    "\n",
    "参考自[官方示例](https://allennlp.org/tutorials).\n",
    "\n",
    "从纯代码的形式使用allennlp，能够很好的理解allennlp在整个处理流程中，不同模块概念在不同流程中的作用。\n",
    "\n",
    "> 我一开始是从 allennlp-train 模式学起，然后其中的配置及其关系各种蒙圈，虽然能改点参数，\n",
    "> 但并不明白其中的流程，于是开始看源码，自己一点一点倒腾，现在倒有点理解，在此分享出来。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 先根据import来看看基本概念\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libs\n",
    "# 建议写代码时，多添些代码类型注释\n",
    "from typing import Iterator, List, Dict\n",
    "\n",
    "# AllenNlp 基于Pytorch编写，所以几乎可以在Allennlp中使用pytorch所有组件\n",
    "# eg：modules，optimizer，operation ......\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Instance\n",
    "\n",
    "顾名思义，每一行文本转化为Instance对象。\n",
    "\n",
    "比如：\n",
    "\n",
    "```javascript\n",
    "instance = Instance({\n",
    "    \"text\"    : TextField([\"I\", \"love\", \"you\"]),\n",
    "    \"label\"   : LabelField(\"happy\"),\n",
    "    \"tags\"    : SequenceLabelField([\"Person\",\"O\",\"Person\"])\n",
    "})\n",
    "```\n",
    "Instance对象中可针对不同任务存储不同格式的数据。\n",
    "比如：文本分类，情感分类等任务，每一个instance都需要一个Label，故将其分类数据存储为LabelField。\n",
    "比如：POS，NER，SlotFilling 等任务中，Instance中的每个Token都需要一个label，故将其分类数据存储为SequenceLabelField。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from allennlp.data import Instance\n",
    "from allennlp.data.fields import TextField, SequenceLabelField"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DatasetReader\n",
    "\n",
    "`dataset_reader` 负责将数据文件读取成一个`Iterable(Instance)`集合。\n",
    "\n",
    "而我们所需要做的核心就是重写`_read(file_path)`函数。\n",
    "\n",
    "因其为`Iterable`对象故可转化为List内存对象，也可作为一个 lazy generator，进行延迟加载数据。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from allennlp.data.dataset_readers import DatasetReader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cache_path\n",
    "\n",
    "如果`file_path`是一个网络地址，则可自动将数据下载到`cache_dir`文件夹下。然后返回本地刚下载好的数据文件路径。\n",
    "\n",
    "否则，直接读取本地文件。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from allennlp.common.file_utils import cached_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenindexer\n",
    "\n",
    "\n",
    "顾名思义，将`Token`转化为`index`，在不同模型中，`word-level`和`character-level`是需要对字符进行不同程度的映射。\n",
    "\n",
    "比如：单词`cat`在`word-level`下的index可能为34。可在`character-level`下的index就可能是[23,10,18]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token\n",
    "from allennlp.data.vocabulary import Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "这个整个Allennlp框架的核心，也是我们最终模型算法的核心部分。\n",
    "\n",
    "Allennlp给我们提供了多种基础模型，开箱即用，比如：CrfTagger，BertForClassification，[BiMpm](https://arxiv.org/abs/1702.03814)，BidirectionalLanguageModel，MaskedLanguageModel ...... \n",
    "\n",
    "哎呀，实在是数不过来，里面有太多提供了开箱即用的模型，希望大家能够多看[源码](https://github.com/allenai/allennlp)，了解其中最新的模型和组件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextFieldEmbedder\n",
    "\n",
    "主要是用于将`Instance`中的`TextField`转化为词向量。\n",
    "\n",
    "**首先**，将`Instance`中的allennlp.data.fields.TextField字段转化为allennlp.data.DataArray对象。\n",
    "\n",
    "**其次**，当我们创建TextField的时候，是有传递一个`Dict`[`str`,`allennlp.data.Tokenindexer`]对象，这样让token使用不同的方式来构建索引。比如:\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "self.token_indexers = {\n",
    "    \"words\": SingleIdTokenIndexer(),\n",
    "    \"characters\":TokenCharactersIndexer()\n",
    "}\n",
    "\n",
    "instance = Instance(\n",
    "    \"text\":TextField([\"I\",\"love\",\"you\"],self.token_indexers),\n",
    "    \"label\":LableField(\"happy\")\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "此时就会使用两种方式来对text中的tokens构建索引\n",
    "\n",
    "最后`Dict[str,TokenEmbedder]`将其转化为词向量，注意 ⚠️ ，此处可有多个TokenEmbedders的key值与token_indexer中的key值相对应，这样不同的token经过指定的tokenindexer后生成索引后，由对应的TokenEmbedder来映射到Embedding。\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "```\n",
    "\n",
    "最后的最后，如果有多种token_indexer/token_embedder，会自动将不同embedding拼接到一起。\n",
    "比如，以下配置：\n",
    "\n",
    "```json\n",
    "\"dataset_reader\": {\n",
    "    \"type\": \"ner_ontonotes\",\n",
    "    \"label_namespace\": \"ontonotes_ner_labels\",\n",
    "    \"coding_scheme\": \"BIOUL\",\n",
    "    \"lazy\": false,\n",
    "    \"token_indexers\": {\n",
    "        \"tokens\": {\n",
    "            \"type\": \"single_id\",\n",
    "            \"lowercase_tokens\": true\n",
    "        },\n",
    "        \"token_characters\": {\n",
    "            \"type\": \"characters\"\n",
    "        },\n",
    "        \"elmo\": {\n",
    "            \"type\": \"elmo_characters\"\n",
    "        }\n",
    "    }\n",
    "},\n",
    "\"model\": {\n",
    "    \"type\": \"ner\",\n",
    "    \"text_field_embedder\": {\n",
    "        \"token_embedders\": {\n",
    "            \"tokens\": {\n",
    "                \"type\": \"embedding\",\n",
    "                \"pretrained_file\": \"./data/glove/glove.6B.100d.txt.gz\",\n",
    "                \"embedding_dim\": 100,\n",
    "                \"trainable\": true\n",
    "            },\n",
    "            \"elmo\": {\n",
    "                \"type\": \"elmo_token_embedder\",\n",
    "                \"options_file\": \"./data/elmo/2x4096_512_2048cnn_2xhighway_options.json\",\n",
    "                \"weight_file\": \"./data/elmo/2x4096_512_2048cnn_2xhighway_weights.hdf5\",\n",
    "                \"do_layer_norm\": false,\n",
    "                \"dropout\": 0,\n",
    "                \"requires_grad\": false\n",
    "            },\n",
    "            \"token_characters\": {\n",
    "                \"type\": \"character_encoding\",\n",
    "                \"embedding\": {\n",
    "                    \"embedding_dim\": 16\n",
    "                },\n",
    "                \"encoder\": {\n",
    "                    \"type\": \"cnn\",\n",
    "                    \"embedding_dim\": 16,\n",
    "                    \"num_filters\": 64,\n",
    "                    \"ngram_filter_sizes\": [\n",
    "                        3\n",
    "                    ]\n",
    "                },\n",
    "                \"dropout\": 0.1\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"ner\": {\n",
    "        \"encoder\": {\n",
    "            \"type\": \"lstm\",\n",
    "            \"bidirectional\": true,\n",
    "            \"input_size\": 1188,\n",
    "            \"hidden_size\": 64,\n",
    "            \"num_layers\": 2,\n",
    "            \"dropout\": 0.2\n",
    "        },\n",
    "        \"tagger\": {\n",
    "            \"label_namespace\": \"ontonotes_ner_labels\",\n",
    "            \"constraint_type\": \"BIOUL\",\n",
    "            \"dropout\": 0.2\n",
    "        }\n",
    "    }\n",
    "},\n",
    "```\n",
    "\n",
    "*注意：token_indexers下的key必须与token_embedder下的key一致。*\n",
    "\n",
    "这里有三种embedding，最后都会拼接成一个input-embedding，便从多种方式下获取特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules\n",
    "\n",
    "这个modules不同与Pytorch中的Module，而是内置来很多已经实现好的Module，提供我们在模型当中使用。\n",
    "\n",
    "比如：多种Attention，多种Seq2SeqEncoder，多种TokenEmbedder，以及ConditionRandomField等等。对于我们复现论文模型和研究算法非常**有用**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.modules.seq2seq_encoders import Seq2SeqEncoder, PytorchSeq2SeqWrapper\n",
    "from allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterators\n",
    "\n",
    "将DatasetReader读取出来的Instance转化为Batch，然后塞给Model。\n",
    "\n",
    "Iterators也有很多类型，不同的数据组装方式对于训练的过程也是有挺大影响的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.iterators import BucketIterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainning\n",
    "\n",
    "对于训练的过程，在Allennlp封装的非常好，只需要一个train方法就可以完成类似于keras中的功能。\n",
    "\n",
    "我最喜欢其中的功能就是：\n",
    "\n",
    "- 自动生成log，这样就可以使用tensorboard查看不同的参数\n",
    "- 自动保存best-checkpoint\n",
    "- 生成良好的训练输出格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x12735f350>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from allennlp.training.metrics import CategoricalAccuracy\n",
    "from allennlp.training.trainer import Trainer\n",
    "from allennlp.predictors import SentenceTaggerPredictor\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开始看代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosDatasetReader(DatasetReader):\n",
    "    \"\"\"\n",
    "    读取数据文件，格式如下：\n",
    "    \n",
    "    The###DET dog###NN ate###V the###DET apple###NN\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, token_indexers: Dict[str, TokenIndexer] = None) -> None:\n",
    "        super().__init__(lazy=False)\n",
    "        # token_indexs 将token映射到指定的索引上\n",
    "        # 如果未指定token_indexers，则默认将每个单词映射到唯一id\n",
    "        self.token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "        \n",
    "    def text_to_instance(self, tokens: List[Token], tags: List[str] = None) -> Instance:\n",
    "        \n",
    "        # 只有TextField上才会传递token_indexers\n",
    "        # LabelField 和 SequenceLabelField 不传递 token_indexers\n",
    "        sentence_field = TextField(tokens, self.token_indexers)\n",
    "        \n",
    "        # fields 最后是转化为Instance\n",
    "        # 同时将文本数据（TextField）放置在 \"sentence\" 键上，所以在模型的forward函数上，就应该有sentence参数。\n",
    "        \n",
    "        fields = {\"sentence\": sentence_field}\n",
    "        # tags这里为什么可能是None？\n",
    "        # 答：在train模式下就会传递tags数据，可如果是在对应的predict模型下，就不会喘息tags。\n",
    "        if tags:\n",
    "            label_field = SequenceLabelField(labels=tags, sequence_field=sentence_field)\n",
    "            fields[\"labels\"] = label_field\n",
    "\n",
    "        return Instance(fields)\n",
    "    def _read(self, file_path: str) -> Iterator[Instance]:\n",
    "        with open(file_path) as f:\n",
    "            for line in f:\n",
    "                pairs = line.strip().split()\n",
    "                sentence, tags = zip(*(pair.split(\"###\") for pair in pairs))\n",
    "                yield self.text_to_instance([Token(word) for word in sentence], tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmTagger(Model):\n",
    "    \"\"\"\n",
    "    最上层的模型，融合多种modules\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 word_embeddings: TextFieldEmbedder,\n",
    "                 encoder: Seq2SeqEncoder,\n",
    "                 vocab: Vocabulary) -> None:\n",
    "        super().__init__(vocab)\n",
    "        \n",
    "        # 用于将token_index转化为embedding\n",
    "        self.word_embeddings = word_embeddings\n",
    "        \n",
    "        # 派生于Seq2SeqEncoder，建议看看allennlp.modules.seq2seq下的多种模型\n",
    "        # 官网上的api文档写的太简单了，看源码你会了解的更多。\n",
    "        self.encoder = encoder\n",
    "        \n",
    "        # allennlp有一个很让人舒适的地方就是：大部分allennlp.modules下的module，都有一个`get_output_dim()`函数，\n",
    "        # 这样在一定程度上减少模型的耦合度和配置复杂性\n",
    "        self.hidden2tag = torch.nn.Linear(in_features=encoder.get_output_dim(),\n",
    "                                          out_features=vocab.get_vocab_size('labels'))\n",
    "        \n",
    "        # 不是Loss，也不是Optimizer，不影响训练的learning-rate或grad\n",
    "        # 而是在训练的过程中，输出训练效果分数，比如：accuracy，f1score，recall-score等\n",
    "        self.accuracy = CategoricalAccuracy()\n",
    "        \n",
    "    def forward(self,\n",
    "                sentence: Dict[str, torch.Tensor],\n",
    "                labels: torch.Tensor = None) -> Dict[str, torch.Tensor]:\n",
    "        \n",
    "        # 这个非常重要，一个batch中不同文本有不同的长度，故需要获取mask来指定参数更新梯度\n",
    "        mask = get_text_field_mask(sentence)\n",
    "        \n",
    "        # 将sentence数据映射到词向量。\n",
    "        \"\"\"\n",
    "        注意，这里 sentence 类型是 Dict[str,torch.Tensor] \n",
    "        \n",
    "        比如token_indexers设置了 word,charaters 两个不同的token_indexer，则此处的sentence也会有这两个key，\n",
    "        \n",
    "        并交给text_field_embedding（也包含word，charaters这两个TokenEmbedder）映射成词向量。\n",
    "        \"\"\"\n",
    "        embeddings = self.word_embeddings(sentence)\n",
    "        \n",
    "        # seq2seq_encoder\n",
    "        encoder_out = self.encoder(embeddings, mask)\n",
    "        \n",
    "        # shape : (batch_size, sequence_length , label_size)\n",
    "        tag_logits = self.hidden2tag(encoder_out)\n",
    "        \n",
    "        \n",
    "        output = {\"tag_logits\": tag_logits}\n",
    "        if labels is not None:\n",
    "            self.accuracy(tag_logits, labels, mask)\n",
    "            output[\"loss\"] = sequence_cross_entropy_with_logits(tag_logits, labels, mask)\n",
    "\n",
    "        return output\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        return {\"accuracy\": self.accuracy.get_metric(reset)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PosDatasetReader()\n",
    "train_dataset = reader.read(cached_path('./data/train.txt'))\n",
    "validation_dataset = reader.read(cached_path('./data/validation.txt'))\n",
    "\n",
    "# 手动构造Vocabulary\n",
    "vocab = Vocabulary.from_instances(train_dataset + validation_dataset)\n",
    "\n",
    "# 自定义超参数\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6\n",
    "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
    "                            embedding_dim=EMBEDDING_DIM)\n",
    "\n",
    "# \"tokens\" 是需要和DataserReader中的token_indexers的 key 保持一致\n",
    "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
    "\n",
    "lstm = PytorchSeq2SeqWrapper(torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\n",
    "model = LstmTagger(word_embeddings, lstm, vocab)\n",
    "if torch.cuda.is_available():\n",
    "    cuda_device = 0\n",
    "    model = model.cuda(cuda_device)\n",
    "else:\n",
    "    cuda_device = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义优化器\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "# 自定义数据迭代方式\n",
    "iterator = BucketIterator(batch_size=2, sorting_keys=[(\"sentence\", \"num_tokens\")])\n",
    "\n",
    "# 现在回想一下，token_indexers 要想将token映射到index，那没有vocabulary如何映射呢？\n",
    "# 这里就是给iterator设置vocab。\n",
    "# 在iterator对数据进行组装的时候，会调用token_indexer函数并将vocab传递过去，此时token_indexer才会接触到vocab。\n",
    "iterator.index_with(vocab)\n",
    "\n",
    "# 这里的方法就很类似于keras的\n",
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  iterator=iterator,\n",
    "                  train_dataset=train_dataset,\n",
    "                  validation_dataset=validation_dataset,\n",
    "                  patience=10,\n",
    "                  num_epochs=1000,\n",
    "                  cuda_device=cuda_device)\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
