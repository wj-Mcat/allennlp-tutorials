![dataset-reader](../assert/allennlp-introduce.png)

# å‰è¨€

æœ¬æ¬¡å°†è¦ä»‹ç»çš„æ˜¯Allennlpæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºPytorchï¼Œé¢å‘æ·±åº¦å­¦ä¹ ä¸­çš„è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„æ¡†æ¶ï¼Œæä¾›äº†ä¼—å¤šçš„æ–°å…´ç®—æ³•å’Œé¢„è®­ç»ƒæ¨¡å‹ï¼Œåªéœ€è¦ç®€å•çš„å‡ è¡Œä»£ç å°±å¯ä»¥å®Œæˆå¾ˆæ£’çš„åŠŸèƒ½ã€‚

æœ¬æ¬¡æ•™ç¨‹ï¼Œé€šè¿‡ç¤ºä¾‹ä»£ç æ¥è®²è§£ä¸åŒæ¨¡å—çš„ä½¿ç”¨æ–¹æ³•å’ŒåŸç†ï¼Œå¸Œæœ›é€šè¿‡æœ¬ç¯‡åšæ–‡ï¼Œå¤§å®¶èƒ½å¤Ÿé¡ºåˆ©ä½¿ç”¨ä¸Š[Allennlp](https://github.com/allenai/allennlp)ï¼Œå› ä¸ºç›¸æ¯”çº¯æ‰‹åŠ¨æ’¸Pytorchï¼ŒAllennlpçœŸçš„èƒ½å¤ŸåŠ é€ŸIdeaçš„å®ç°ã€‚

> å‚è€ƒè®ºæ–‡ï¼š[AllenNLP: A Deep Semantic Natural Language Processing Platform](https://arxiv.org/abs/1803.07640)

# ä»‹ç»

Allennlpå°†NLPä»»åŠ¡å¤„ç†æµç¨‹ä¸­çš„å„ä¸ªé˜¶æ®µéƒ½åšäº†ä¸€å®šç¨‹åº¦çš„æŠ½è±¡ï¼Œåœ¨è½¯ä»¶è®¾è®¡ä¸Šè®²å°±æ˜¯ï¼Œå®ç°äº†é«˜å†…èšï¼Œä½è€¦åˆï¼Œè®©æˆ‘ä»¬èƒ½å¤Ÿä¸“æ³¨äºç‰¹å®šæ¨¡å—çš„é€»è¾‘ï¼Œè€Œæ— éœ€å…¶ä»–æµç¨‹çš„æ”¹åŠ¨ï¼Œæå¤§ç¨‹åº¦ä¸Šå‡å°‘äº†å·¥ä½œé‡ã€‚

é‚£å¸¸ç”¨ä¸”é‡è¦çš„å¤„ç†æµç¨‹æœ‰ï¼š

- DatasetReaderï¼šä»æ–‡ä»¶ä¸­è¯»å–æ•°æ®ï¼Œè½¬åŒ–ä¸ºInstanceé›†åˆ
- Modelï¼šæ¨¡å‹ä¸»ä½“
- Iteratorï¼šè¿­ä»£æ•°æ®ï¼Œæå–batchæ•°æ®
- Trainerï¼šæ¨¡å‹è®­ç»ƒå™¨ï¼Œå¹¶è®°å½•metric
- Predictorï¼šä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹æ¥é¢„æµ‹æ•°æ®


ä»¥ä¸Šæ¯ä¸ªpipelineæ˜¯æ¾è€¦åˆçš„ï¼Œæ¯”å¦‚è¯´ä»DatasetReaderè¯»å–çš„vocab_sizeï¼Œinput_embedding,ç­‰ä¸éœ€è¦å•ç‹¬é…ç½®ï¼Œè€Œæ˜¯å¯ä»¥é€šè¿‡Vocabularyä¸­çš„æ¥å£è€Œè·å–ã€‚

æ¯ä¸ªNLPä»»åŠ¡ï¼Œéƒ½æ˜¯ä»æ•°æ®é¢„å¤„ç†å¼€å§‹ï¼Œæˆ‘ä»¬å°±å…ˆä»DatasetReaderå¼€å§‹å°†ï¼Œç„¶åé¡ºç€æ•°æ®çš„å¤„ç†æµç¨‹æ¥è®²è§£å…¶ä¸­ä¸åŒçš„æ¦‚å¿µ ...

## DatasetReader

æ•°æ®é¢„å¤„ç†ï¼Œç¹çæ— èŠä½†åˆå°‘ä¸äº†ï¼Œè€ŒAllennlpè®©æˆ‘ä»¬åªå…³æ³¨äºæ ¸å¿ƒçš„æ•°æ®è¯»å–ï¼Œå…¶ä»–æ— èŠçš„äº‹æƒ…éƒ½å¸®æˆ‘ä»¬åšå¥½ï¼Œé€šç”¨çš„ä¸œè¥¿ç»å¯¹ä¸è®©æˆ‘ä»¬é‡å¤ç¼–ç ï¼Œå› æ­¤æˆ‘ä»¬åªéœ€è¦å®Œæˆå¾®ä¹å‡ å¾®çš„é€»è¾‘å¤„ç†ï¼Œæ¯”å¦‚åœ¨DatasetReaderæˆ‘ä»¬åªéœ€è¦å®ç°ä¸¤ä¸ªå‡½æ•°å³å¯ï¼š_read , text_to_instanceï¼Œå…¶å†…éƒ¨å®ç°çš„åŠŸèƒ½å¦‚ä¸‹ï¼š

1. ä»æœ¬åœ°è¯»å–æ•°æ®
2. ä»æ•°æ®ä¸­è¯»å–ç›¸å…³æ•°æ®å­—æ®µ
3. å°†æå–çš„æ•°æ®è½¬åŒ–æˆInstanceæ•°ç»„

![dataset-reader](../assert/allennlp-dataset-reader.png)

ç¤ºä¾‹ä»£ç å¦‚ä¸‹ï¼š

```python
@DatasetReader.register("pos")
class PosDatasetReader(DatasetReader):
    """
    DatasetReader for PoS tagging data, one sentence per line, like
    The###DET dog###NN ate###V the###DET apple###NN
    """
    def __init__(self, token_indexers: Dict[str, TokenIndexer] = None) -> None:
        super().__init__(lazy=False)
        self.token_indexers = token_indexers or {"tokens": SingleIdTokenIndexer()}
    def text_to_instance(self, tokens: List[Token], tags: List[str] = None) -> Instance:
        sentence_field = TextField(tokens, self.token_indexers)
        fields = {"sentence": sentence_field}

        if tags:
            label_field = SequenceLabelField(labels=tags, sequence_field=sentence_field)
            fields["labels"] = label_field

        return Instance(fields)
    def _read(self, file_path: str) -> Iterator[Instance]:
        with open(file_path) as f:
            for line in f:
                pairs = line.strip().split()
                sentence, tags = zip(*(pair.split("###") for pair in pairs))
                yield self.text_to_instance([Token(word) for word in sentence], tags)
```

âœ¨ ä»£ç å¾ˆç®€å•ã€‚æ³¨æ„è¿™é‡Œçš„text_to_instanceå‡½æ•°æ˜¯é‡å†™ï¼Œä¹Ÿå¯ä»¥å°†text_to_instanceä¸­çš„ä»£ç æ¬è¿åˆ°_readä¸­ï¼Œç¨‹åºä¸ä¼šå‡ºé”™ï¼Œåªä¸è¿‡é‰´äºé€»è¾‘åˆ†ç¦»çš„è§„å®šï¼Œå»ºè®®å°†ä¸åŒé€»è¾‘çš„ä»£ç ä½¿ç”¨ä¸åŒå‡½æ•°è¿›è¡Œéš”ç¦»ã€‚

ğŸ¤” è¿™ä¸ªæ—¶å€™ï¼Œæœ‰å¿ƒäººä¼šæ³¨æ„åˆ°ï¼ŒDatasetReader å’Œ Pytorchä¸­çš„Datasetã€DataLoaderçš„å¼‚åŒç‚¹æ˜¯å•¥ï¼Ÿ

ğŸ™‹ ç­”ï¼šDatasetReaderæ˜¯å¯¹åä¸¤è€…çš„èåˆã€‚

- ç›¸åŒç‚¹
    - éƒ½å®Œæˆæ•°æ®çš„è¯»å–ä¸æŠ½è±¡çš„åŠŸèƒ½
    - éƒ½èƒ½å¤Ÿå®ç°æ•°æ®çš„æ‡’åŠ è½½ï¼ˆé’ˆå¯¹äºå¤§æ•°æ®é‡çš„æƒ…å†µï¼‰
    - éƒ½æœ‰åŠ é€Ÿæ•°æ®è¯»å–çš„æ€§èƒ½çš„æœºåˆ¶
- ä¸åŒç‚¹
    - DatasetReaderåˆäºŒä¸ºä¸€ï¼Œæ›´åŠ ç®€å•æ˜“æ‡‚
    - DatasetReaderåªéœ€è¦å®ç°ä¸€ä¸ªå‡½æ•°å³å¯å®Œæˆæ‰€æœ‰åŠŸèƒ½
    - DatasetReaderèƒ½å¤Ÿåœ¨æ•°æ®ç±»å‹ä¸Šæ”¯æŒå¤šç§ä¸åŒNLPä»»åŠ¡
    - å…¶ä»–çš„æˆ‘éƒ½ä¸è¯´äº†ï¼Œåæ­£å¾ˆç®€å•......ğŸ¤¡

è¯´äº†ä¸åŒç‚¹ï¼Œå¯è¿˜æ²¡æœ‰è¯´åˆ°å…¶æ ¸å¿ƒå¼ºå¤§ä¹‹å¤„ã€‚æˆ‘å°†ä»¥ä¸‰ç‚¹è¯´ï¼š
- æ‡’åŠ è½½æ•°æ®
- çº¦å®šèƒœä¸é…ç½®
- ä¸åŒç±»å‹çš„Field

### æ‡’åŠ è½½

åœ¨ç¼–å†™_readå‡½æ•°çš„æ—¶å€™ï¼Œä½¿ç”¨yieldå…³é”®å­—è¿”å›æ•°æ®ï¼ŒDatasetReaderäºæ˜¯å¯¹äºæ•°æ®çš„è¯»å–å¤©ç„¶æœ‰ç€ä¸€ç§lazy generatoræ¨¡å¼ï¼Œæ”¯æŒæ‡’åŠ è½½æ•°æ®ï¼Œå¦‚æœè¦å¼€å¯lazyæ¨¡å¼ï¼Œåªéœ€è¦åœ¨ä¼ é€’lazyå‚æ•°å°±è¡Œã€‚

å½“ç„¶ï¼Œå¯¹å†…å­˜æœ‰æ‰€ä¼˜åŒ–ï¼Œåœ¨æ—¶é—´ä¸Šå°±ä¼šæœ‰æ‰€æ¶ˆè€—ï¼Œé’ˆå¯¹ä¸åŒçš„æ•°æ®é›†ä½¿ç”¨ä¸åŒçš„æ•°æ®åŠ è½½æ¨¡å¼ã€‚

### çº¦å®šèƒœä¸é…ç½®

ç»†å¿ƒçš„æœ‹å‹ä¼šå‘ç°ï¼Œæˆ‘ä»¬é‡å†™çš„æ˜¯_readå‡½æ•°ï¼Œè¿™æ˜¯ä¸€ä¸ªä¼¼æœ‰å‡½æ•°çš„åç§°ï¼Œä¹Ÿæ˜¯æ•´ä¸ªæ•°æ®è¯»å–æœ€æ ¸å¿ƒçš„å‡½æ•°ã€‚æˆ‘ä»¬åªéœ€è¦å®ç°è¿™ä¸ªå‡½æ•°ï¼Œå…¶ä»–æ•°æ®æ•´ç†ä¸è¯»å–è¿›åº¦æ¡çš„å±•ç¤ºç­‰é™„å¸¦åŠŸèƒ½å°±ä¼šè‡ªåŠ¨å¸®æˆ‘ä»¬å®Œæˆã€‚

## ä¸åŒç±»å‹çš„Field

DatasetReaderæœ€ç»ˆè¿”å›çš„æ˜¯Instanceå®ä¾‹çš„é›†åˆï¼Œè€ŒInstanceå®é™…ä¸Šæ˜¯ä¸€ä¸ªå­—å…¸ç±»å‹çš„æ•°æ®ï¼š`MutableMapping[str, Field]`ï¼Œvalueæ˜¯Fieldç±»å‹çš„æ•°æ®ï¼Œå¸¸ç”¨çš„Fieldç±»å‹æœ‰ï¼š

- TextField
- LabelField
- SequenceLabelField
- KnowledgeGraphField
- ...

é¦–å…ˆï¼ŒFieldçš„ä½œç”¨å°±æ˜¯å°†tokenè½¬åŒ–æˆå¯¹åº”çš„indexï¼Œä¸”ä¸åŒå®ç°æœ‰ä¸åŒçš„å¤„ç†æ–¹æ³•ï¼Œè¯¦ç»†å¯ä»¥å»çœ‹[å®˜æ–¹ä»£ç ](https://github.com/allenai/allennlp)ã€‚

è¿™é‡Œæˆ‘å°±ä¸ä¸€ä¸€ä»‹ç»æ¥ï¼Œç®€å•ä»‹ç»å‡ ä¸ªã€‚

å…ˆçœ‹TextFieldï¼Œç”¨æ¥å­˜å‚¨åºåˆ—ï¼ˆSequenceFieldï¼‰æ•°æ®ï¼Œåœ¨åˆå§‹åŒ–çš„æ—¶å€™æ˜¯éœ€è¦ä¼ é€’token_indexersï¼Œè¿™ä¸ªå¯ä»¥å¯¹æ•°æ®è¿›è¡Œä¸åŒç¨‹åº¦çš„åˆ†è¯å¤„ç†ã€‚

è€ŒLabelFieldå’ŒSequenceLabelFieldå°±ä¸éœ€è¦token_indexersï¼Œå› ä¸ºæœ¬èº«å°±æ˜¯labelæ ‡ç­¾ï¼Œæ˜¯ä¸éœ€è¦åˆ†è¯å¤„ç†ã€‚

ğŸ¤” å¥½äº†ï¼Œæˆ‘ä»¬çŸ¥é“äº†Fieldæ˜¯å¦‚ä½•å­˜å‚¨æ•°æ®ï¼Œé‚£å¦‚ä½•æŠŠå­—æ®µå¡è¿›model.forwardå‡½æ•°ä¸­å»å‘¢ï¼Ÿ

ğŸ™‹ ç­”ï¼šInstanceä¸­çš„keyå€¼ä¸forwardå‡½æ•°ä¸­çš„å‚æ•°åå¯¹åº”ã€‚

æ¯”å¦‚ä»¥ä¸Šä»£ç ä¸­Instanceä¸­æœ‰sentenceå’Œlabelå­—æ®µï¼Œåˆ™åœ¨model.forwardå‡½æ•°ä¸­å°±åº”è¯¥æœ‰å¯¹åº”çš„å‚æ•°ï¼Œå¦‚ï¼š

```python
@Model.register("lstm-tagger")
class LstmTagger(Model):

    # å…¶ä»–å‡½æ•°æˆ‘å°±ä¸å±•ç¤ºå‡ºæ¥äº†
    def forward(self,
                sentence: Dict[str, torch.Tensor],
                labels: torch.Tensor = None) -> Dict[str, torch.Tensor]:
        pass
```

çœ‹è§æ²¡ï¼ŒInstanceå’Œmodel.forwardå‡½æ•°çš„å‚æ•°å°±æ˜¯è¿™æ ·å¯¹åº”çš„ã€‚

ã€æ€è€ƒé¢˜ã€‘ï¼šå’³å’³ï½ï½ï¼Œç»†å¿ƒçš„æœ‹å‹å¯èƒ½ä¼šè§‚å¯Ÿåˆ°ï¼Œsentenceçš„æ•°æ®ç±»å‹æ˜¯`Dict[str, torch.Tensor]`ï¼Œå¯æˆ‘åœ¨Instanceä¸­sentenceå­˜å‚¨çš„å°±æ˜¯TextFieldï¼Œå‰é¢ä¹Ÿè¯´äº†ï¼ŒTextFieldæœ¬èº«å°±æ˜¯éœ€è¦å°†æ•°æ®è½¬åŒ–æˆindexï¼Œç„¶åç”¨torch.tensoråŒ…è£…ä¸‹å°±æˆäº†torch.Tensoræ•°æ®ç±»å‹ï¼Œé‚£ä¸ºä»€ä¹ˆä¼šæ˜¯Dict[str,torch.Tensor]æ•°æ®ç±»å‹å‘¢ï¼Ÿ
ã€çº¿ç´¢ã€‘ï¼štoken_indexers , æ€è€ƒäº”ç§’é’Ÿ......


## token_indexers & token_embedders

ä¸ºäº†è®²æ¸…æ¥šè¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ä¾¿è·Ÿéšç€sentenceè¿™ä¸ªå­—æ®µä»è¯»å–åˆ°æ˜ å°„æˆè¯å‘é‡è¿™æ•´ä¸ªæµç¨‹æ¥è®²è§£ã€‚

sentenceå­—æ®µçš„å¤„ç†åˆ†ä¸ºä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š
1. tokenizer -> åˆ†è¯
2. Token     -> è½¬åŒ–ä¸ºå•ä¸ªTokenå¯¹è±¡
3. Instance  -> è½¬åŒ–ä¸ºInstanceå®ä¾‹
4. Iterator  -> å¹¶ç»„è£…æˆbatchæ¨¡å¼
5. model.forward -> å¡ç»™æ¨¡å‹å»æ‰§è¡Œ
6. token_embedders -> å°†idxè½¬åŒ–æˆè¯å‘é‡

1. **tokenizer -> åˆ†è¯**

è¿™ä¸ªè¿‡ç¨‹æ˜¯åœ¨æ–‡æœ¬è¯»å–çš„æ—¶å€™æ‰§è¡Œçš„ã€‚åœ¨DatasetReaderåˆå§‹åŒ–çš„æ—¶å€™ï¼Œä¼šå°†tokenizerä¼ é€’åˆ°æ„é€ å‡½æ•°å½“ä¸­ï¼Œæ²¡æœ‰çš„è¯å°±åˆå§‹åŒ–ä¸€ä¸ªé»˜è®¤åˆ†è¯å™¨ã€‚è¿™ä¸ªåˆ†è¯å™¨å¯ä»¥æ˜¯ä¸€ä¸ªï¼Œä¹Ÿå¯ä»¥æ˜¯å¤šä¸ªï¼Œå–å†³äºåœ¨å‚æ•°åˆ—è¡¨é‡Œé¢ä¼ é€’çš„ä¸ªæ•°ã€‚

å¯¹äºè‹±æ–‡åˆ†è¯ï¼Œallennlpæœ‰å†…ç½®çš„WordTokenizerï¼Œå¯æ˜¯ä¸­æ–‡åˆ†è¯çš„è¯ï¼Œå°±éœ€è¦è‡ªå·±æ‰‹åŠ¨æ„é€ ä¸€ä¸ªï¼šç»§æ‰¿Tokenizerï¼Œç„¶åæ³¨å†Œã€‚è¿™æ ·å°±å¯ä»¥åœ¨é…ç½®æ–‡ä»¶ä¸­é€šè¿‡typeæ‰¾åˆ°å®šåˆ¶åˆ†è¯å™¨ã€‚

```text
"I love cat" -> ["I", "love", "cat"]
```

2. **Token  -> è½¬åŒ–æˆå•ä¸ªTokenå¯¹è±¡**

å¦‚æœçœ‹è¿‡Tokenæºç å°±ä¼šçŸ¥é“ï¼Œå…¶æ ¸å¿ƒå­˜å‚¨ç€textï¼Œtext_id,åˆ†åˆ«ä»£è¡¨ç€åˆ†è¯çš„**æ–‡æœ¬**ä»¥åŠ**ç´¢å¼•**ã€‚

è¿™ä¸ªè¿‡ç¨‹æ¯”è¾ƒç®€å•ï¼Œæ²¡æœ‰ä»€ä¹ˆé€»è¾‘ã€‚

```text
["I", "love", "cat"] -> [Token("I"), Token("love"), Token("cat")]
```

3. **Instance -> è½¬åŒ–ä¸ºInstanceå®ä¾‹**

è¿™ä¸ªè¿‡ç¨‹ä¸€èˆ¬æ˜¯åœ¨DatasetReaderçš„text_to_instanceå‡½æ•°ä¸­å®Œæˆï¼Œå¹¶é’ˆå¯¹ä¸åŒå­—æ®µè½¬åŒ–æˆä¸åŒçš„Fieldã€‚

```python
tokens = [Token("I"), Token("love"), Token("cat")]
token_indexers = {
    "word_token": SingleIdTokenIndexer(),
    "character_token": TokenCharactersIndexer()
}
instance = {
    "sentence": TextField(tokens, token_indexers)
}
```

4. **Iterator -> ç»„è£…æˆbatchæ¨¡å¼**

è¿™ä¸ªè¿‡ç¨‹æˆ–è®¸çœ‹ä¸è§ï¼Œå¯æ˜¯é€»è¾‘åŸºæœ¬ä¸Šå›ºå®šï¼Œå¦‚æ— ç‰¹æ®Šéœ€æ±‚ï¼Œæ— éœ€å®šåˆ¶ã€‚

å°†Instanceè½¬åŒ–æˆidxçš„ä¼ªä»£ç å¦‚ä¸‹æ‰€ç¤ºï¼š

```python
instance = {
    "sentence": {
        "word_token": ["I", "love", "cat"] -> torch.Tensor([23, 55, 67]),
        "character_token": [["I"], ["l", "o", "v", "e"],["c", "a", "t"]] -> torch.Tensor([[23], [34, 78, 35, 36],[13, 74, 26]])
    }
}
```

ç„¶è€Œï¼ŒIteratorçœ‹ä¼¼ç®€å•ï¼Œå¯è¿˜æœ‰ä¸€äº›ç»†èŠ‚æˆ‘æƒ³ä¸å¤§å®¶èŠèŠï¼š

- åœ¨batchæ•°æ®çš„æ—¶å€™ï¼ŒåŒbatchä¸­ä¸åŒé•¿åº¦çš„æ•°æ®æ˜¯éœ€è¦pad
- ä¸ºäº†padè¿‡ç¨‹çš„**æ€§èƒ½**ï¼Œå¯ä¼˜å…ˆå°†é•¿åº¦ç›¸è¿‘çš„æ–‡æœ¬æ”¾ç½®åœ¨åŒä¸€ä¸ªbatchä¸­
- éšæœºæ‰“ä¹±æ•°æ®

Allennlpå·²ç»å†…ç½®äº†å‡ ä¸ªDataIteratorï¼Œå‡ ä¹ä¸éœ€è¦ä½ è‡ªå·±é‡å†™ï¼Œé™¤éä½ åœ¨batchçš„è¿‡ç¨‹ä¸­ï¼Œå®Œæˆä¸€äº›åˆ›æ–°æ€§çš„å°trickã€‚

ç¤ºä¾‹ä»£ç å¦‚ä¸‹ï¼š

```python
from allennlp.data.iterators import BucketIterator
 
iterator = BucketIterator(batch_size=config.batch_size, 
                          biggest_batch_first=True,
                          sorting_keys=[("tokens", "num_tokens")],
                         )
iterator.index_with(vocab)
```

- sorting_keys èƒ½å¤Ÿæå‡paddingè¿‡ç¨‹æ•ˆç‡ã€‚
- index_with(vocab)éå¸¸é‡è¦ï¼šç»™token_indexersé…ç½®vocabularyã€‚**è¿™ä¸€æ­¥åƒä¸‡ä¸è¦ç»™å¿˜è®°äº†**ä¸ºä»€ä¹ˆè¦è¿™æ ·åšå‘¢ï¼Ÿ
    1. token_indexersæ˜¯åœ¨dataset_readeråˆå§‹åŒ–çš„æ—¶å€™æ‰å­˜åœ¨çš„ï¼Œè€Œvocabularyæ˜¯éœ€è¦åŸºäºdataset_readerè¯»å–çš„Instanceé›†åˆæ‰èƒ½å¤Ÿè¿›è¡Œæ„å»ºçš„ï¼Œæ•…æ­¤å¤„çŸ›ç›¾ï¼Œæ— æ³•æŒ‡å®šã€‚
    2. token_indexerså¹¶éä¸èƒ½æå–æˆä¸€ä¸ªå•ç‹¬çš„æ¨¡å—æ¥æŒ‡å®šVocabularyï¼Œå¯ä»è½¯ä»¶è®¾è®¡çš„è§’åº¦æ¥çœ‹ï¼ŒFieldä¾èµ–äºtoken_indexersï¼Œéœ€è¦åœ¨åˆå§‹åŒ–çš„æ—¶å€™å°±æŒ‡å®šï¼Œæ•…æ— æ³•è®¾è®¡æˆä¸€ä¸ªå•ç‹¬çš„æ¨¡å—ã€‚
    3. åœ¨iteratorä¸­æŒ‡å®švocabularyï¼Œç„¶åç”±iteratorå°†å…¶ä¼ é€’ç»™token_indexerçš„tokens_to_indicesè¿™ä¸ªå‡½æ•°ï¼Œæ­¤å¤„çš„ä¸€ä¸ªå°trickå°±è§£å†³æ¥ä¾èµ–æ€§çš„ä¸€ä¸ªé—®é¢˜ã€‚


5. **model.forward -> æ¨¡å‹çš„å‚æ•°**

Instanceç»è¿‡token_indexersè½¬åŒ–æˆç´¢å¼•ä¹‹åï¼Œç”±Iteratorç»„è£…æˆbatchæ•°æ®ï¼Œç„¶åå¡ç»™æ¨¡å‹çš„forwardå‡½æ•°ã€‚

```python

token_embedders = {
    "word_token": TokenEmbedder(embedding_dim = 23),
    "character_token": TokenEmbedder(embedding_dim = 27)
}

text_field_embedders = BaseTextFieldEmbedder(token_embedders)

def forward(self,sentence: Dict[str,torch.Tensor]):
    sentence_embedding = text_field_embedders(sentence)
    
```

ä¸Šè¿°ä¼ªä»£ç å¾ˆç®€å•çš„ï¼Œä¸è¿‡éœ€è¦æ³¨æ„å‡ ç‚¹ï¼š
1ï¼‰text_field_embedderså‚æ•°token_embeddersçš„å…³é”®å­—å’Œtoken_indexersçš„å…³é”®å­—å¿…é¡»è¦ä¿æŒä¸€è‡´ã€‚
2ï¼‰å¤šç§TokenEmbedderså¯¹åŒä¸€ä¸ªæ–‡æœ¬åˆ†åˆ«åšå¤„ç†å¹¶æ˜ å°„åˆ°è¯å‘é‡åï¼Œå°†å…¶æ‹¼æ¥åˆ°ä¸€èµ·ã€‚æ¯”å¦‚ä¸Šè¿°ä¸¤ä¸ªtoken_embedderç»´åº¦ä¸º23å’Œ27ï¼Œsentence_embeddingçš„ç»´åº¦å°±ä¸º50ã€‚é€šè¿‡ç®€å•çš„å‡ è¡Œä»£ç å°±å¯ä»¥å®Œæˆå¾ˆå¤æ‚çš„è¯å‘é‡æ‹¼æ¥çš„åŠŸèƒ½ã€‚

Awesomeï½ï½

6. **token_embedders -> è¯å‘é‡æ˜ å°„**

å…¶å®å¦‚ä½•å°†å°†æ–‡æœ¬ç´¢å¼•æ˜ å°„åˆ°è¯å‘é‡ï¼Œç¬¬äº”ç‚¹å°±å·²ç»è¯´äº†ã€‚å…¶æ ¸å¿ƒéœ€è¦æ³¨æ„çš„å°±æ˜¯ï¼š
- token_indexerså’Œtoken_embedderséƒ½æ˜¯å­—å…¸ç±»å‹ï¼Œä¸”é”®å€¼å¿…é¡»ä¿æŒä¸€è‡´
- token_embedderså¤„ç†åçš„è¯å‘é‡æ˜¯æ‹¼æ¥åˆ°ä¸€èµ·**ï¼ˆè¿™ä¸ªç‰¹æ€§éå¸¸æ£’ï¼‰**

è‡³æ­¤ï¼Œæˆ‘ä»¬è·Ÿéšç€sentenceå­—æ®µä»è¯»å–åˆ°æ˜ å°„æˆè¯å‘é‡æ•´ä¸ªæµç¨‹éƒ½å·²ç»èŠå®Œäº†ï¼Œç›¸ä¿¡éƒ½å·²ç»æŒæ¡äº†ã€‚

è‡³æ­¤ï¼Œæˆ‘å·²ç»å°†æ¨¡å‹æ‰§è¡Œä¹‹å‰æ‰€æœ‰çš„æ³¨æ„ç‚¹éƒ½ç»™èŠå®Œäº†ã€‚


## Model

å»ºè®®çœ‹çœ‹æºç ï¼Œå› ä¸ºçœ‹äº†æºç ä½ æ‰ä¼šå‘ç°ï¼ŒAllennlpçš„modelå’Œmoduleéƒ½æ˜¯åŸºäºPytorchçš„torch.nn.Moduleæ¨¡å—å»ºç«‹ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥å¾ˆå®¹æ˜“çš„ä½¿ç”¨Allennlpä¸­çš„ä»»ä½•ç±»ã€‚

![dataset-reader](../assert/allennlp-dataset-reader.png)

ä¸ºäº†è¯´æ˜Allennlpä¸­çš„æ¨¡å‹ï¼Œæˆ‘å…ˆä¸Pytorchä¸­çš„æ¨¡å‹åšä¸€ä¸ªå¯¹æ¯”è¯´æ˜ï¼š

ç¤ºä¾‹ä»£ç å¦‚ä¸‹ï¼š
```python
def forward(self, tokens: Dict[str, torch.Tensor],
                id: Any, label: torch.Tensor) -> torch.Tensor:
        mask = get_text_field_mask(tokens)
        embeddings = self.word_embeddings(tokens)
        state = self.encoder(embeddings, mask)
        class_logits = self.projection(state)
         
        output = {"class_logits": class_logits}
        output["loss"] = self.loss(class_logits, label)
 
        return output
```

- Allennlpä¸­çš„forwardå‡½æ•°å‚æ•°æœ‰çš„æ˜¯ä¸€ä¸ªå­—å…¸ç±»å‹ï¼ˆç±»ä¼¼äºTextFieldæŒ‡å®šäº†TokenIndexerï¼‰çš„æ•°æ®ï¼Œæœ‰çš„æ˜¯çº¯torch.Tensoræ•°æ®ã€‚è€ŒPytorchä¸­çš„æ•°æ®æ ¼å¼æ²¡æœ‰é™åˆ¶ï¼Œè‡ªç”±åº¦ç”±è‡ªå·±æ§åˆ¶ï¼Œä½†æ¨èæ˜¯torch.Tensoræ•°æ®ç±»å‹ã€‚
- Allennlpforwardå‡½æ•°è¿”å›çš„ä¹Ÿæ˜¯ä¸€ä¸ªå­—å…¸ç±»å‹çš„æ•°æ®ï¼Œå…¶ä¸­æœ€é‡è¦çš„å°±æ˜¯æŸå¤±å‡½æ•°å€¼ï¼Œå¿…é¡»å°†å…¶å­˜å‚¨åœ¨lossé”®ä¸‹ï¼ˆè¿™æ˜¯ä¸€ä¸ª**çº¦å®š**ï¼‰ï¼ŒåŒæ—¶losså€¼çš„ä¸€ä¸ªè®¡ç®—ä¹Ÿæ˜¯åœ¨forwardå‡½æ•°ä¸­æ‰§è¡Œçš„ã€‚

è‡³æ­¤ï¼Œæˆ‘ä»¬å·²ç»äº†è§£ä¸ºä»€ä¹ˆforwardå‡½æ•°æœ‰çš„å‚æ•°ä¸ºä»€ä¹ˆæ˜¯å­—å…¸ç±»å‹ã€‚é‚£æˆ‘è¿™é‡Œæƒ³é—®ä¸€ä¸ªé—®é¢˜ï¼š

ä¸ºä»€ä¹ˆforwardå‡½æ•°è¿”å›çš„å€¼ä¹Ÿæ˜¯å­—å…¸ç±»å‹ï¼Ÿ
ç­”ï¼šå› ä¸ºåœ¨Trainerä¸­æœ‰å¤§ç”¨å¤„ï¼Œåé¢æˆ‘ä¼šè¿›ä¸€æ­¥è®²è§£ã€‚

Allennlpä¸ºä»€ä¹ˆä¼šè¿™ä¹ˆå¥½ç”¨å‘¢ï¼Ÿ

ç­”ï¼šå› ä¸ºé‡Œé¢å°†å¾ˆå¤šç±»ä¼¼çš„ç»„ä»¶ï¼Œå¹¶å°†å…¶æŠ½è±¡æˆæ¨¡å—æ¥ä½¿ç”¨ï¼Œå¸¸ç”¨çš„æœ‰ï¼š

- token_embedder
- encoder
- decoder

ç¤ºä¾‹ä»£ç å¦‚ä¸‹ï¼š

```python
from allennlp.nn.util import get_text_field_mask
from allennlp.models import Model
from allennlp.modules.text_field_embedders import TextFieldEmbedder
 
class BaselineModel(Model):
    def __init__(self, word_embeddings: TextFieldEmbedder,
                 encoder: Seq2VecEncoder,
                 out_sz: int=len(label_cols)):
        super().__init__(vocab)
        self.word_embeddings = word_embeddings
        self.encoder = encoder
        self.projection = nn.Linear(self.encoder.get_output_dim(), out_sz)
        self.loss = nn.BCEWithLogitsLoss()
         
    def forward(self, tokens: Dict[str, torch.Tensor],
                id: Any, label: torch.Tensor) -> torch.Tensor:
        mask = get_text_field_mask(tokens)
        embeddings = self.word_embeddings(tokens)
        state = self.encoder(embeddings, mask)
        class_logits = self.projection(state)
         
        output = {"class_logits": class_logits}
        output["loss"] = self.loss(class_logits, label)
 
        return output
```

ä¸Šé¢ä»£ç ä¸­ï¼Œencoderæ˜¯Seq2VecEncoderï¼ˆåŸºç±»ï¼‰ç±»å‹ï¼Œæ•…å¯æ›´æ”¹encoderçš„å®ç°ï¼Œå¹¶ä¸æ”¹å˜æ¨¡å‹å†…éƒ¨çš„ä»£ç ï¼Œå®ç°ç»„ä»¶çš„å•ç‹¬æ›¿æ¢ï¼Œè€Œæˆ‘ä»¬æ‰€éœ€è¦åšçš„ï¼Œåªéœ€è¦æ·»åŠ Seq2VecEncoderçš„æ´¾ç”Ÿç±»å³å¯ã€‚

è¿™å°±æ˜¯Allennlpçš„å¼ºå¤§ä¹‹å¤„ï¼šæ‰€æœ‰çš„æµç¨‹éƒ½æŠ½è±¡åŒ–ï¼Œå…·ä½“å®ç°åªéœ€è¦è‡ªå·±æŒ‡å®šæˆ–å®ç°å°±è¡Œã€‚

## Train

æ¥ä¸‹æ¥å°±è¯¥è®¨è®ºæ¨¡å‹çš„**è®­ç»ƒæµç¨‹**äº†ã€‚

ç›¸æ¯”Pytorchç¹æ‚ä¸”æ¯«æ— æ–°ç‰¹æ€§çš„è®­ç»ƒè¿‡ç¨‹ï¼ŒKeraså’Œtensorflowæ¡†æ¶å°±åšçš„å¾ˆå¥½ï¼Œåªéœ€è¦ç®€å•çš„å‡ è¡Œä»£ç å°±å¯ä»¥æ›¿ä»£Pytorchå¤šè¡Œæ‰‹åŠ¨ç¼–åˆ¶çš„è®­ç»ƒloopã€‚

AllennlpæŠ½è±¡äº†ä¸€ä¸ªTrainerï¼Œç”¨æ¥æ‰§è¡Œè®­ç»ƒè¿‡ç¨‹ï¼šæ›´æ–°æ¢¯åº¦ï¼Œä¿å­˜æ—¥å¿—æ–‡ä»¶ï¼ˆç”¨tensorboardæŸ¥çœ‹ï¼‰ï¼Œä¿å­˜best_modelï¼Œè‰¯å¥½çš„è®­ç»ƒè¿‡ç¨‹è¾“å‡ºç­‰ç­‰ï¼Œè¿™ä¸ªè®­ç»ƒå™¨å¤§å¤§å‡å°‘äº†æˆ‘ä»¬çš„å·¥ä½œé‡ã€‚

ç¤ºä¾‹ä»£ç å¦‚ä¸‹ï¼Œä¹Ÿæ˜¯éå¸¸ç®€å•ï¼š

```python
from allennlp.training.trainer import Trainer
 
trainer = Trainer(
    model=model,
    optimizer=optim.Adam(model.parameters(), lr=config.lr),
    iterator=iterator,
    train_dataset=train_ds,
    cuda_device=0 if USE_GPU else -1,
    num_epochs=config.epochs,
)

trainer.train()
```

é…ç½®å®ŒTrainerï¼Œåªéœ€è¦æŒ‡å®štrainå‡½æ•°å³å¯å®Œæˆè®­ç»ƒçš„æ•´ä¸ªè¿‡ç¨‹ã€‚

# æ€»ç»“

Allennlpéå¸¸å¥½ç”¨ï¼Œä¹Ÿæœ‰è¶³å¤Ÿçš„å®šåˆ¶åŒ–èƒ½åŠ›ï¼Œä¹Ÿå¯ä»¥å’ŒTransformerå®Œç¾ç»“åˆåœ¨ä¸€èµ·ï¼ŒåŒæ—¶ä¹Ÿæ”¯æŒ**semantic parsing**ï¼Œ**state machine**ï¼Œæœªæ¥ä¹Ÿä¼šæ·»åŠ æ›´å¤šæ›´å¼ºå¤§çš„æ¨¡å—ã€‚

[allennlp-tutorials](https://github.com/wj-Mcat/allennlp-tutorials)


# å‚è€ƒé“¾æ¥ï¼š

- [an-in-depth-tutorial-to-allennlp-from-basics-to-elmo-and-bert](http://mlexplained.com/2019/01/30/an-in-depth-tutorial-to-allennlp-from-basics-to-elmo-and-bert/)
- [AllenNLP: A Deep Semantic Natural Language Processing Platform](https://arxiv.org/abs/1803.07640)